from typing import List, Optional
from langchain_openai import ChatOpenAI
from tavily import TavilyClient
from pydantic import BaseModel, Field, field_validator
from langchain_community.chat_models import ChatPerplexity
from serpapi import GoogleSearch
from exa_py import Exa
from langchain.output_parsers import PydanticOutputParser, RetryOutputParser
from langchain.prompts import PromptTemplate
from langsmith import traceable
import json
import cohere
import requests
from urllib.parse import quote_plus
import concurrent.futures
from concurrent.futures import ThreadPoolExecutor
import requests
from utils import retry
import os
from dotenv import load_dotenv
from ScraperAPIWrapper import ScraperAPIWrapper
from utils import deduplicate_candidates, get_page_ranks

load_dotenv()

tavily = TavilyClient(api_key=os.getenv('TAVILY_API_KEY'))
exa = Exa(api_key=os.getenv('EXA_API_KEY'))
gpt_4o_model = ChatOpenAI(model="gpt-4o", openai_api_key=os.getenv('OPENAI_API_KEY'))
gpt_4o_mini_model = ChatOpenAI(model="gpt-4o-mini", openai_api_key=os.getenv('OPENAI_API_KEY'))
perplexity_model = ChatPerplexity(pplx_api_key=os.getenv('PERPLEXITY_API_KEY'), model="llama-3.1-sonar-large-128k-online")
co = cohere.ClientV2(os.getenv('COHERE_API_KEY'))
is_dev = os.getenv('IS_DEV') == 'true'

scraper_api_url = os.getenv("SCRAPER_API_URL")
scraper = ScraperAPIWrapper(scraper_api_url)

class Candidate(BaseModel):
    title: str = Field(description="Only the title or name of the exact product specific to the given query")
    product_URL: str = Field(description="The landing page URL specific to the product represented in the title and relevant to the query")
    overview: str = Field(description="A crisp, consice, packed, dense, very deep descriptive overview of the product, relevant to the query. atleast use five sentences to cover as many details as possible")
    raw_content: Optional[str] = Field(
        default=None, description="In each and every case and Always skip this optional field"
    )
    source: Optional[str] = Field(
        default=None, description="In each and every case and Always skip this optional field"
    )

    # Custom validator to ensure 'description' is not empty
    @field_validator('overview')
    def description_cannot_be_empty(cls, v):
        if not v.strip():  # Checks if the string is empty or contains only whitespace
            raise ValueError('Description cannot be empty.')
        return v

    @field_validator('product_URL')
    def url_cannot_be_invalid(cls, v):
        response = requests.get(v, timeout=30)
        if response.status_code == 404:
            raise ValueError('Invalid URL.')
        return v

    @field_validator('title')
    def title_cannot_be_empty(cls, v):
        if not v.strip():  # Checks if the string is empty or contains only whitespace
            raise ValueError('Title cannot be empty.')
        return v

class CandidateList(BaseModel):
    candidates: List[Candidate] = Field(description="A list of Candidates")

class ExtractProductDocument(BaseModel):
    title: str = Field(description="title or name of the product/tool/company/candidate")
    description: str = Field(description="A short, crisp, consice, packed descriptive summary of the product/tool/company/candidate")
    inputs: List[str] =  Field(description="A list of inputs for the product/tool/company/candidate, that are required/optional for this product to work, input parameters")
    outputs: List[str] =  Field(description="A list of outputs for the product/tool/company/candidate, that are generated by this product. output attributes")
    process: str = Field(description="A detailed description of how exatly this tool works, what is its soution process, how does it transforms the inputs to outputs. what technologies does it use")
    examples: List[str] = Field(description="A list of potential examples of how this product can be used, use cases, example scenario")
    pricing: str = Field(description="cost information like pricing model, cost, charges, subscription plans")

parser = PydanticOutputParser(pydantic_object=CandidateList)

@retry(max_tries=3, delay=1, backoff=2, retry_enabled=True, default_value=CandidateList(candidates=[]))
@traceable(run_type="retriever")
def get_candidates_perplexity(query: str, model=perplexity_model):
    """Finds the potential candidates from the world web/internet using perplexity."""

    prompt = PromptTemplate(
        template=""" You are a researcher, you have to help the human with a list of relevant products/tools/company/candidates to their query, by thoroughly searching the web/internet. and extract the title, home URL of that actual product, overview.
        Make sure the product_URL is the URL of the actual product and absoluetly not the url of the article talking about the product.
        {format_instructions}
        {query}""",
        input_variables=["query"],
        partial_variables={"format_instructions": parser.get_format_instructions()},
        )

    prompt_and_model = prompt | model
    result = prompt_and_model.invoke({"query": query})
    json_str = result.content.split('```json')[1].split('```')[0]
    json_str = json.loads(json_str)
    candidates = json_str['candidates']
    url_query_list = []
    for c in candidates:
        url_query_list.append("https://www.google.com/search?q=" + quote_plus(c['title']))
    raw_content = scraper.scrape_multiple(url_query_list)
    c_list = []
    for c in raw_content:
        c = "google result page content:" + c[0] + "\n helpful URLs: \n" + "\n".join(c[1]) + "\n"
        c_list.append(c)
    raw_content = c_list
    # print(raw_content)
    raw_content = "\n #### \n".join(raw_content)
    raw_content = "use the following content scraped from the google search page for to find the most appropriate product_URL for each of the candidates \n" + raw_content
    raw_content = PromptTemplate(template=raw_content)
    retry_parser = RetryOutputParser.from_llm(parser=parser, llm=gpt_4o_mini_model)
    result = retry_parser.parse_with_prompt(result.content, raw_content)

    # print(candidates)
    # print(parser.get_format_instructions())
    # print(result)
    # result = parser.invoke(result)
    
    for c in result.candidates:
        c.source = "perplexity"
        if is_dev:
            c.title = "(perp)" + c.title
    return result

@retry(max_tries=3, delay=1, backoff=2, retry_enabled=True, default_value=CandidateList(candidates=[]))
@traceable(run_type="retriever")
def get_candidates_tavily(query: str, model=gpt_4o_mini_model):
    """Finds the potential candidates from the world web/internet using tavily."""
    content = []
    response = tavily.search(query=query, max_results=5, include_raw_content=True, include_answer=False)
    helpful_urls = []
    for r in response["results"]:
        helpful_urls.append(r['url'])

    helpful_urls = scraper.scrape_multiple(helpful_urls)
    for i, r in enumerate(response['results']):
        content.append(str(r['raw_content']) + "\n helpful URLs: \n" + "\n".join(helpful_urls[i][1]) + "\n" + str(r['url']))
    content = "\n #### \n".join(content)

    prompt = PromptTemplate(
        template=""" From the below content Extract a list of most relevant products/tools/candidates to the given query.

        Query:
        {query}

        Content:
        {content}

        """,
        input_variables=["query", "content"]
        )

    prompt_and_model = prompt | model.with_structured_output(CandidateList)
    result = prompt_and_model.invoke({"query": query, "content": content})
    for c in result.candidates:
        c.source = "tavily"
        if is_dev:
            c.title = "(tavily)" + c.title
        
    return result

@retry(max_tries=3, delay=1, backoff=2, retry_enabled=True, default_value=CandidateList(candidates=[]))
@traceable(run_type="retriever")
def get_candidates_google(query: str, model=gpt_4o_mini_model):
    """Finds the potential candidates from the world web/internet using Google search."""

    search = GoogleSearch({
        "q": query,
        "location": "Austin,Texas",
        "api_key": os.getenv('GOOGLE_SERP_API_KEY')
    })
    result = search.get_dict()

    print(result)

    urls = []
    for i, r in enumerate(result['organic_results']):
        urls.append(r['link'])
        if i >= 4:
            break
    try:
        for i, r in enumerate(result['ads']):
            urls.append(r['link'])
            if i >= 4:
                break
    except:
        pass

    response = scraper.scrape_multiple(urls)

    content = []
    for i, r in enumerate(response):
        content.append(str(r[0]) + "\n helpful URLs: \n" + "\n".join(r[1]) + "\n" + str(urls[i]))

    content = "\n #### \n".join(content)

    prompt = PromptTemplate(
        template=""" From the below content Extract a list of most relevant products/tools/candidates to the given query.

        Query:
        {query}

        Content:
        {content}

        """,
        input_variables=["query", "content"]
        )

    prompt_and_model = prompt | model.with_structured_output(CandidateList)
    result = prompt_and_model.invoke({"query": query, "content": content})
    
    for c in result.candidates:
        c.source = "google"
        if is_dev:
            c.title = "(google)" + c.title
    return result

@retry(max_tries=3, delay=1, backoff=2, retry_enabled=True, default_value=CandidateList(candidates=[]))
@traceable(run_type="retriever")
def get_candidates_exa(query: str, model = None):
    """Finds the potential candidates from the world web/internet using Exa."""

    result = exa.search_and_contents(
        query, use_autoprompt=True, num_results=25, summary=True
    )
    if is_dev:
        prefix = "(exa)"
    else:
        prefix = ""

    candidates = []
    for r in result.results:
        try:
            candidates.append(Candidate(title=prefix + r.title, product_URL=r.url, overview=r.summary, source="exa"))
        except:
            pass
    if len(candidates) < 5:
        raise(ValueError('few candidates got', len(candidates)))
    return CandidateList(candidates=candidates)

def select_relevant_links(query: str, baseURL: str, URLs: list, model):
    class URLList(BaseModel):
        URLs: List[str] = Field(description="A list of relevant URLs")

    model = model.with_structured_output(URLList)
    result = model.invoke(query + "\n".join(URLs))
    result = result.URLs

    return result

def create_document(URL: str, model):

    sublinks = scraper.scrape(URL)[1]
    query = f"""Given the following sublinks for the baseURL '{URL}'. select and extract the most important unique nineteen URLs that are important to understand the product and it capabilities.
     and also help us understand thing like what? how? who? where? when?, requirements, integrations, about us, team, features, pricing, usecases, examples, customers, API Spec, documentation and many more such things.
     ####
     Sublinks:
     """
    sublinks = select_relevant_links(query, URL, sublinks, model)
    sublinks = sublinks[:5]
    try:
        response = scraper.scrape_multiple(sublinks + [URL])
    except Exception as e:
        print(f"An error occurred: {e}")
        return None

    content = []
    for r in response:
        content.append(r[0])
    content = "\n #### \n".join(content)

    prompt = PromptTemplate(
        template=""" Analyse the following text corpus from the product website and extract the following feilds. be accurate, factual and grounded.

        Content:
        {content}

        """,
        input_variables=["content"]
        )

    prompt_and_model = prompt | model.with_structured_output(ExtractProductDocument)
    result = prompt_and_model.invoke({ "content": content})
    result = json.loads(result.json())
    result['raw_content'] = content

    return result

def create_document_wrapper(r):
    # Wrapper function to extract relevant data and create document
    document = create_document(r['document']['product_URL'], gpt_4o_mini_model)
    if document is None:
        return None
    document['product_URL'] = r['document']['product_URL']
    document['overview'] = r['document']['overview']
    return document

@traceable(run_type="retriever")
def get_candidates(query: str):
    with ThreadPoolExecutor() as executor:
        futures = [
            executor.submit(get_candidates_perplexity, query, perplexity_model),
            executor.submit(get_candidates_tavily, query, gpt_4o_mini_model),
            executor.submit(get_candidates_google, query, gpt_4o_mini_model),
            executor.submit(get_candidates_exa, query)
        ]

        # Collect the results as they complete
        all_candidates = []
        for future in concurrent.futures.as_completed(futures):
            all_candidates.extend(future.result().candidates)

    all_candidates_original = CandidateList(candidates=all_candidates)
    all_candidates = json.loads(all_candidates_original.json())
    all_candidates = all_candidates['candidates']
    all_candidates = deduplicate_candidates(all_candidates)
    print(all_candidates)

    landing_page_urls = []
    for c in all_candidates:
        landing_page_urls.append(c['product_URL'])

    landing_pages = scraper.scrape_multiple(landing_page_urls)
    page_ranks = get_page_ranks(landing_page_urls)

    for i, c in enumerate(all_candidates):
        all_candidates[i]['raw_content'] = landing_pages[i][0]
        all_candidates_original.candidates[i].raw_content = landing_pages[i][0]

    results = co.rerank(model="rerank-english-v3.0", query=query, documents=all_candidates, rank_fields=['overview', 'raw_content'])
    results = json.loads(results.json())
    results = results['results']

    if page_ranks != []:
        # Multiply each relevance score with the respective page rank using the index
        for i, result in enumerate(results):
            result['adjusted_score'] = result['relevance_score'] * page_ranks[result['index']]

        # Sort the items based on the multiplied score in decreasing order
        results.sort(key=lambda x: x['adjusted_score'], reverse=True)

    print(results)

    results = results[:14]

    selected_candidates = []
    for r in results:
        selected_candidates.append(all_candidates_original.candidates[r['index']])

    selected_candidates = CandidateList(candidates=selected_candidates)

    # documents = []
    # print('creating documents')
    # with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    #     documents = list(executor.map(create_document_wrapper, results))

    # documents = [d for d in documents if d is not None]
    return selected_candidates


def get_comparison_matrix(documents, requirements, model):

    class ProductCapability(BaseModel):
        capability: str = Field(description="reasoning of the product capability for meeting the corresponding requirement")
        score: int = Field(description="a score on how well it meets the corresponding requirement")

        # Custom validator to ensure 'description' is not empty
        @field_validator('score')
        def score_validity(cls, v):
            if (v > 5) or (v < 0):  # Checks if the score is valid, it need to be out of 5
                raise ValueError('Invalid Score')
            return v

    class ProductCapabilites(BaseModel):
        Capabilites: List[ProductCapability] = Field(description="A list of Product Capabilites in the same order with respective to requirements, same number of items as requirements")

        # Custom validator to ensure 'description' is not empty

        @field_validator('Capabilites')
        def check_number_of_capabilites(cls, v):
            if len(v) != len(requirements):  # Checks if the required number if Capabilites are generated
                raise ValueError('Capabilites missed')
            return v

    prompt = PromptTemplate(
        template=""" By understanding the detailed document of a product given below.

        Document:
        {document}

        Answer if this prodcut can meet the following requirements given below with a detailed reasoning of capability. be honest, accurate, factual and grounded by above facts.
        If this product can't meet any requirements, lack capabilities or lack of information be completely transparent and admit it.
        and give a score on how well it meets the each requirement. with a score out of five.
        For each and every requirements mentioned below, respond with a respective reasoning of capability and a score for each.

        Requirements:

        {requirements}

        """,
        input_variables=["document", "requirements"]
        )

    prompt_and_model = prompt | model.with_structured_output(ProductCapabilites)

    for document in documents:
        result = prompt_and_model.invoke({"document": str(document), "requirements": "/n - ".join(requirements)})
        result = json.loads(result.json())
        print(result)
        for i, req in enumerate(requirements):
            result['Capabilites'][i]['requirement'] = req
        document['capabilites'] = result['Capabilites']

    return documents